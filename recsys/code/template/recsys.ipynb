{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rcsys.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Content Aware RecSys\n",
        "Your task is to create a system that can recommend movies to users based on the dataset found [here](https://www.kaggle.com/rounakbanik/the-movies-dataset/data). You will need to login to kaggle to download the data. The enriched MovieLens dataset contains the following files:\n",
        "\n",
        "* movies_metadata.csv: The main Movies Metadata file. Contains information on 45,000 movies featured in the Full MovieLens dataset. Features include posters, backdrops, budget, revenue, release dates, languages, production countries and companies.\n",
        "* keywords.csv: Contains the movie plot keywords for our MovieLens movies. Available in the form of a stringified JSON Object.\n",
        "* credits.csv: Consists of Cast and Crew Information for all our movies. Available in the form of a stringified JSON Object.\n",
        "* links.csv: The file that contains the TMDB and IMDB IDs of all the movies featured in the Full MovieLens dataset.\n",
        "* links_small.csv: Contains the TMDB and IMDB IDs of a small subset of 9,000 movies of the Full Dataset.\n",
        "* ratings_small.csv: The subset of 100,000 ratings from 700 users on 9,000 movies.\n",
        "* ratings.csv: The full ratings dataset.\n",
        "\n",
        "Besides having the usual user-interaction data, this dataset also has some textual metadata (either in the `metadata.csv` file or in the `keywords.csv` file). Whatever you use as part of your recommendation system, you must use some form of textual data as a feature in your model.\n",
        "\n",
        "We have defined a helper `recsys.py` file that would help you. It has some boiler plate defined but you need to fill out yourself and write some extra functions or two. You can also solve this using another programming language like Scala , the python code might help you by providing some pseudo-code.\n",
        "\n",
        "If you use extra libraries, please amend the provided `requirements.txt` file and this readme with instructions. Once everything is ready, we could use the tool by running:\n",
        "\n",
        "`python recsys.py --in-folder <path-to-data> --out-folder <path-to-model-destination>` , where\n",
        "\t* `<path-to-data>` corresponds to the data containing the csv files\n",
        "\t* `<path-to-model-destination>` corresponds to a folder where the trained model will be serialised to\n",
        "\n",
        "Your code in `recsys.py` should:\n",
        "* generate a training / eval / test split (and do any necessary data pre-processing)\n",
        "* train a model\n",
        "* print evaluation metrics\n",
        "* save it to a destination\n",
        "\n",
        "Once you are done, submit a pull request for evaluation."
      ],
      "metadata": {
        "id": "o9PbfTL933JA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fire"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwOBfIW32rwU",
        "outputId": "b394dc47-0bf4-4874-9740-af14fa2cb584"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 34.4 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 36.7 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30 kB 38.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 37.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 51 kB 26.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61 kB 29.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 71 kB 31.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 32.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=5b28baa8cc98f073ac1bd6d529f2d0fb155548ee702bb01a0592ce7b6a8dcc8a\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2HgqDmtr2Q2Z"
      },
      "outputs": [],
      "source": [
        "import fire"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(in_folder: str):\n",
        "    \"\"\"\n",
        "    Load the csv file and join them in a format you can use\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "tG2H3R3K2_ii"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_data(data):\n",
        "    \"\"\"\n",
        "    Generate data splits\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "99vL5kg_3APT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_data):\n",
        "    \"\"\"\n",
        "    Evaluate your model against the test data.\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "OqNoAo6C3AYQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(model, out_folder: str):\n",
        "    \"\"\"\n",
        "    Serialise the model to an output folder \n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "9gCXfgWo3AhD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(in_folder: str, out_folder: str) -> None:\n",
        "    \"\"\"\n",
        "    Consume the data from the input folder to generate the model \n",
        "    and serialise it to the out_folder\n",
        "    \"\"\"\n",
        "    pass"
      ],
      "metadata": {
        "id": "Iw5DhHtp2jEq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "if __name__ == '__main__':\n",
        "  fire.Fire(train)\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OqL9aPmY3Gbg",
        "outputId": "f81636f0-9f63-47e9-9445-b79cb401671f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nif __name__ == '__main__':\\n  fire.Fire(train)\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}